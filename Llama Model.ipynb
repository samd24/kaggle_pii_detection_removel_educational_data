{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2baeef",
   "metadata": {},
   "source": [
    "https://www.packtpub.com/article-hub/fine-tuning-llama-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806f2ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\carth\\appdata\\local\\temp\\pip-req-build-kb0_ebn7\n",
      "  Resolved https://github.com/huggingface/transformers to commit ece1b62b93cde70233f235f6a4c84e37bfc8eba0\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (0.20.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (3.9.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers==4.39.0.dev0) (22.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\carth\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.39.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests->transformers==4.39.0.dev0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests->transformers==4.39.0.dev0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests->transformers==4.39.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests->transformers==4.39.0.dev0) (2023.5.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\carth\\AppData\\Local\\Temp\\pip-req-build-kb0_ebn7'\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68aa2f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\carth\\anaconda3\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (0.20.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (4.39.0.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (1.23.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (0.27.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (22.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (2.2.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (4.64.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\carth\\anaconda3\\lib\\site-packages (from peft) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\carth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\carth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.17.0->peft) (2.28.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\carth\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\carth\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\carth\\anaconda3\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers->peft) (0.15.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from transformers->peft) (2022.7.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.5.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca38914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\carth\\anaconda3\\lib\\site-packages (2.17.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: filelock in c:\\users\\carth\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\carth\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\carth\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\carth\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b24499",
   "metadata": {},
   "source": [
    "Original imports\n",
    "\n",
    "# Importing Libraries\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM \n",
    "import torch\n",
    "from datasets import \n",
    "Dataset import transformers \n",
    "import pandas as pd\n",
    "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_int8_training, get_peft_model_state_dict, PeftModel \n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a5c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers \n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from peft import get_peft_model, LoraConfig, TaskType, get_peft_model_state_dict, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import LlamaTokenizer, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f111a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SMSSpamCollection.csv', sep='\\t', header=None, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21138a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v1,v2,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham,\"Go until jurong point, crazy.. Available ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham,Ok lar... Joking wif u oni...,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam,Free entry in 2 a wkly comp to win FA Cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham,U dun say so early hor... U c already then...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>spam,\"This is the 2nd time we have tried 2 con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham,Will Ì_ b going to esplanade fr home?,,,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>ham,\"Pity, * was in mood for that. So...any ot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>ham,The guy did some bitching but I acted like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5574</th>\n",
       "      <td>ham,Rofl. Its true to its name,,,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5575 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0                                              v1,v2,,,\n",
       "1     ham,\"Go until jurong point, crazy.. Available ...\n",
       "2                  ham,Ok lar... Joking wif u oni...,,,\n",
       "3     spam,Free entry in 2 a wkly comp to win FA Cup...\n",
       "4     ham,U dun say so early hor... U c already then...\n",
       "...                                                 ...\n",
       "5570  spam,\"This is the 2nd time we have tried 2 con...\n",
       "5571       ham,Will Ì_ b going to esplanade fr home?,,,\n",
       "5572  ham,\"Pity, * was in mood for that. So...any ot...\n",
       "5573  ham,The guy did some bitching but I acted like...\n",
       "5574                  ham,Rofl. Its true to its name,,,\n",
       "\n",
       "[5575 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f24add35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  \\\n",
      "0                                           v1,v2,,,   \n",
      "1  ham,\"go until jurong point, crazy.. available ...   \n",
      "2               ham,ok lar... joking wif u oni...,,,   \n",
      "3  spam,free entry in 2 a wkly comp to win fa cup...   \n",
      "4  ham,u dun say so early hor... u c already then...   \n",
      "\n",
      "                       output_text  \n",
      "0                               {}  \n",
      "1  {'wat...\",,,': '10_WORDS_LONG'}  \n",
      "2                               {}  \n",
      "3  {'rate)t&c's': '10_WORDS_LONG'}  \n",
      "4                               {}  \n"
     ]
    }
   ],
   "source": [
    "all_text = df[0].str.lower().tolist()  \n",
    " \n",
    "input, output = [], []  \n",
    "for text in all_text:  \n",
    "               input.append(text)  \n",
    "               output.append({word: '10_WORDS_LONG' for word in text.split() if len(word)==10}) \n",
    " \n",
    "df = pd.DataFrame([input, output]).T \n",
    "df.rename({0:'input_text', 1: 'output_text'}, axis=1, inplace=True) \n",
    "print (df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea92418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham,\"go until jurong point, crazy.. available ...</td>\n",
       "      <td>{'wat...\",,,': '10_WORDS_LONG'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham,ok lar... joking wif u oni...,,,</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam,free entry in 2 a wkly comp to win fa cup...</td>\n",
       "      <td>{'rate)t&amp;c's': '10_WORDS_LONG'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham,u dun say so early hor... u c already then...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham,\"nah i don't think he goes to usf, he live...</td>\n",
       "      <td>{'though\",,,': '10_WORDS_LONG'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>spam,\"this is the 2nd time we have tried 2 con...</td>\n",
       "      <td>{'spam,\"this': '10_WORDS_LONG'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham,will ì_ b going to esplanade fr home?,,,</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham,\"pity, * was in mood for that. so...any ot...</td>\n",
       "      <td>{'ham,\"pity,': '10_WORDS_LONG'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5572</th>\n",
       "      <td>ham,the guy did some bitching but i acted like...</td>\n",
       "      <td>{'interested': '10_WORDS_LONG'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5573</th>\n",
       "      <td>ham,rofl. its true to its name,,,</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5574 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input_text  \\\n",
       "0     ham,\"go until jurong point, crazy.. available ...   \n",
       "1                  ham,ok lar... joking wif u oni...,,,   \n",
       "2     spam,free entry in 2 a wkly comp to win fa cup...   \n",
       "3     ham,u dun say so early hor... u c already then...   \n",
       "4     ham,\"nah i don't think he goes to usf, he live...   \n",
       "...                                                 ...   \n",
       "5569  spam,\"this is the 2nd time we have tried 2 con...   \n",
       "5570       ham,will ì_ b going to esplanade fr home?,,,   \n",
       "5571  ham,\"pity, * was in mood for that. so...any ot...   \n",
       "5572  ham,the guy did some bitching but i acted like...   \n",
       "5573                  ham,rofl. its true to its name,,,   \n",
       "\n",
       "                          output_text  \n",
       "0     {'wat...\",,,': '10_WORDS_LONG'}  \n",
       "1                                  {}  \n",
       "2     {'rate)t&c's': '10_WORDS_LONG'}  \n",
       "3                                  {}  \n",
       "4     {'though\",,,': '10_WORDS_LONG'}  \n",
       "...                               ...  \n",
       "5569  {'spam,\"this': '10_WORDS_LONG'}  \n",
       "5570                               {}  \n",
       "5571  {'ham,\"pity,': '10_WORDS_LONG'}  \n",
       "5572  {'interested': '10_WORDS_LONG'}  \n",
       "5573                               {}  \n",
       "\n",
       "[5574 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(0, axis=0, inplace=True)\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7ea8190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76311f1bd6564dce9874755bc6afeaf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48ee9b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc03bb3dfbc445896fe1932af8cac48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    " \n",
    "\n",
    "def create_peft_config(model): \n",
    "    peft_cofig = LoraConfig( \n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.05, \n",
    "    target_modules=['q_proj', 'v_proj'],) \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model.enable_input_require_grads() \n",
    "    model = get_peft_model(model, peft_cofig) \n",
    "    model.print_trainable_parameters() \n",
    "    return model, peft_cofig\n",
    " \n",
    "model, lora_config = create_peft_config(model) \n",
    " \n",
    "def generate_prompt(data_point): \n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. \n",
    "    ### Instruction: \n",
    "    Extract entity from the given input: \n",
    "    ### Input: \n",
    "    {data_point[\"input_text\"]} \n",
    "    ### Response: \n",
    "    {data_point[\"output_text\"]}\"\"\"\n",
    "\n",
    "\n",
    "tokenizer.pad_token_id = 0 \n",
    "def tokenize(prompt, add_eos_token=True): \n",
    "    result = tokenizer( \n",
    "    prompt, \n",
    "    truncation=True, \n",
    "    max_length=128, \n",
    "    padding=False, \n",
    "    return_tensors=None,\n",
    "    ) \n",
    "    if ( result[\"input_ids\"][-1] != tokenizer.eos_token_id and len(result[\"input_ids\"]) < 128 and add_eos_token):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1) \n",
    "        result[\"labels\"] = result[\"input_ids\"].copy() \n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point): \n",
    "    full_prompt = generate_prompt(data_point) \n",
    "    tokenized_full_prompt = tokenize(full_prompt) \n",
    "    return tokenized_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c948898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58d42e586e942238b8f65ac6c609df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637fb08ce87e4bff8c40b96a5c35f2ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_ds = shuffle(df, random_state=42) \n",
    "total_train_ds = total_ds.head(4000) \n",
    "total_test_ds = total_ds.tail(1500)\n",
    "\n",
    "train_examples = {\"input_text\": [], \"output_text\": []}\n",
    "test_examples = {\"input_text\": [], \"output_text\": []}\n",
    "\n",
    "for _, row in total_train_ds.iterrows():\n",
    "    train_examples[\"input_text\"].append(row[\"input_text\"])\n",
    "    train_examples[\"output_text\"].append(row[\"output_text\"])\n",
    "\n",
    "for _, row in total_test_ds.iterrows():\n",
    "    test_examples[\"input_text\"].append(row[\"input_text\"])\n",
    "    test_examples[\"output_text\"].append(row[\"output_text\"])\n",
    "\n",
    "total_train_ds_hf = Dataset.from_dict(train_examples)\n",
    "total_test_ds_hf = Dataset.from_dict(test_examples)\n",
    "\n",
    "tokenized_tr_ds = total_train_ds_hf.map(generate_and_tokenize_prompt) \n",
    "tokenized_te_ds = total_test_ds_hf.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0add6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carth\\anaconda3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mTrainer(model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[0;32m     14\u001b[0m             train_dataset\u001b[38;5;241m=\u001b[39mtokenized_tr_ds, eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_te_ds, args\u001b[38;5;241m=\u001b[39mtraining_arguments, \n\u001b[0;32m     15\u001b[0m                data_collator\u001b[38;5;241m=\u001b[39mdata_collator) \n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m): \n\u001b[1;32m---> 18\u001b[0m        \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2943\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m-> 2943\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2945\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2946\u001b[0m         )\n\u001b[0;32m   2947\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[0;32m   2948\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "training_arguments = transformers.TrainingArguments(  \n",
    "per_device_train_batch_size=1, gradient_accumulation_steps=16,  \n",
    "learning_rate=4e-05,  \n",
    "logging_steps=100,  \n",
    "optim=\"adamw_torch\",  \n",
    "evaluation_strategy=\"steps\",  \n",
    "save_strategy=\"steps\",  \n",
    "eval_steps=100,  \n",
    "save_steps=100,  \n",
    "output_dir=\"saved_models/\" \n",
    ") \n",
    "data_collator = transformers.DataCollatorForSeq2Seq(tokenizer) \n",
    "trainer = transformers.Trainer(model=model, tokenizer=tokenizer, \n",
    "            train_dataset=tokenized_tr_ds, eval_dataset=tokenized_te_ds, args=training_arguments, \n",
    "               data_collator=data_collator) \n",
    " \n",
    "with torch.autocast(\"cuda\"): \n",
    "       trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ccc2621",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2943\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2942\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m-> 2943\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2944\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2945\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(outputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2946\u001b[0m         )\n\u001b[0;32m   2947\u001b[0m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[0;32m   2948\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the TrainingArguments\n",
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=4e-05,\n",
    "    logging_steps=100,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    output_dir=\"saved_models/\"\n",
    ")\n",
    "\n",
    "# Create a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_tr_ds,\n",
    "    eval_dataset=tokenized_te_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print details of the first 5 examples in the tokenized training dataset\n",
    "for i in range(min(5, len(tokenized_tr_ds))):  \n",
    "    print(f\"Example {i + 1}:\")\n",
    "    try:\n",
    "        print(\"Input IDs:\", tokenized_tr_ds[i][\"input_ids\"])\n",
    "        print(\"Attention Mask:\", tokenized_tr_ds[i][\"attention_mask\"])\n",
    "        if \"labels\" in tokenized_tr_ds[i]:\n",
    "            print(\"Labels:\", tokenized_tr_ds[i][\"labels\"])\n",
    "        else:\n",
    "            print(\"Labels not available\")\n",
    "    except KeyError as e:\n",
    "        print(\"Error:\", e)  # Print the specific error if a key is missing\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_tr_ds[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
